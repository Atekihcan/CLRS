---
title:       Exercise 5.1-3
published:   2025-01-08 10:10
modified:    2025-01-08 10:10
keywords:    "unbiased random, biased random, python code"
description: "[Python code] Suppose that you want to output 0 with probability 1/2 and 1 with probability 1/2. At your disposal is a procedure BIASED-RANDOM, that outputs either 0 or 1. Give an algorithm that uses BIASED-RANDOM as a subroutine, and returns an unbiased answer. What is the expected running time as a function of p?"
---

> Suppose that you want to output 0 with probability $$1/2$$ and 1 with probability $$1/2$$. At your disposal is a procedure $$\textsc{Biased-Random}$$, that outputs either 0 or 1. It outputs 1 with some probability $$p$$ and 0 with probability $$1 - p$$, where $$0 < p < 1$$, but you do not know what $$p$$ is. Give an algorithm that uses $$\textsc{Biased-Random}$$ as a subroutine, and returns an unbiased answer, returning 0 with probability $$1/2$$ and 1 with probability $$1/2$$. What is the expected running time of your algorithm as a function of $$p$$?

The key insight is to call $$\textsc{Biased-Random}$$ **twice** and observe the pattern of results.

#### The Clever Trick

If we call $$\textsc{Biased-Random}$$ twice:

$$\begin {align*}
P(00) &= (1-p) \cdot (1-p) = (1-p)^2 \\
P(01) &= (1-p) \cdot p = p(1-p) \\
P(10) &= p \cdot (1-p) = p(1-p) \\
P(11) &= p \cdot p = p^2
\end {align*}$$

Note that probabilities of $$P(01)$$ and $$P(10)$$ are **equal**, regardless of the value of $$p$$. This gives us our unbiased source:

- If we get $$(0,1)$$, return 0
- If we get $$(1,0)$$, return 1
- If we get $$(0,0)$$ or $$(1,1)$$, discard and try again

{% capture code %}
while true
    x = Biased-Random()
    y = Biased-Random()
    if x = 0 and y = 1
        return 0
    if x = 1 and y = 0
        return 1
{% endcapture %}
{% include clrs_code.html title="Unbiased-Random()" %}

#### Analysis of Expected Running Time

Each iteration makes 2 calls to $$\textsc{Biased-Random}$$ and takes $$\Theta(1)$$ time.

The probability of accepting a pair (returning a value) is:

$$P(\text{accept}) = P(01) + P(10) = 2p(1-p)$$

The expected number of iterations is:

$$E[\text{iterations}] = \frac{1}{P(\text{accept})} = \frac{1}{2p(1-p)}$$

Each iteration makes 2 calls to $$\textsc{Biased-Random}$$, so the expected number of calls is:

$$E[\text{calls}] = \frac{2}{2p(1-p)} = \frac{1}{p(1-p)}$$

Therefore, the expected running time is $$\Theta(1/[p(1-p)])$$.

#### Understanding the Running Time

The function $$p(1-p)$$ is maximized when $$p = 1/2$$ (giving $$p(1-p) = 1/4$$), and approaches 0 as $$p$$ approaches 0 or 1.

- **Best case**: When $$p = 1/2$$, $$E[\text{calls}] = 4$$
- **Worst case**: When $$p$$ is very close to 0 or 1, we need many iterations

For example:
- If $$p = 0.9$$, then $$E[\text{calls}] = 1/(0.9 \cdot 0.1) = 11.11$$
- If $$p = 0.99$$, then $$E[\text{calls}] = 1/(0.99 \cdot 0.01) = 101.01$$

{% capture note %}
This algorithm was discovered independently by several people in the 1950s and is sometimes called [**von Neumann's trick**](https://en.wikipedia.org/wiki/Fair_coin#Fair_results_from_a_biased_coin){:target="_blank"}. It's a beautiful example of how symmetry can be exploited to eliminate bias.

The same principle can be extended: if you don't know $$p$$ but want to simulate a coin with probability $$q$$, you can still use this technique as a building block!
{% endcapture %}
{% include aside.html title='A Classic Algorithm' %}

#### Python Implementation

{% include code/code.html file='code/05/code_E050103.py' %}
