---
title:       Exercise 7.1-3
published:   2025-11-16 10:00
modified:    2025-11-16 10:00
keywords:    "quicksort, partition, running time, complexity analysis"
description: "Give a brief argument that the running time of PARTITION on a subarray of size n is Θ(n)."
---

> Give a brief argument that the running time of $$\textsc{Partition}$$ on a subarray of size $$n$$ is $$\Theta(n)$$.

The $$\textsc{Partition}$$ procedure performs a constant amount of work for each element in the subarray, making its running time linear.

Let's examine the procedure's structure. Lines 1–2 perform constant-time initialization, setting the pivot $$x$$ and index $$i$$. The **for** loop in lines 3–6 is the heart of the algorithm. It executes exactly $$r - p$$ times (once for each element from $$p$$ to $$r - 1$$). Since the subarray has $$n = r - p + 1$$ elements, the loop runs $$n - 1$$ times.

Inside the loop, each iteration does constant work: one comparison (line 4), and possibly an increment (line 5) and a swap (line 6). Swapping two array elements takes $$\Theta(1)$$ time regardless of whether it actually occurs. After the loop, lines 7–8 perform one final swap and a return, both taking constant time.

We can express the total running time as:

$$\begin{align*}
T(n) &= \Theta(1) + (n-1) \cdot \Theta(1) + \Theta(1) \\
     &= \Theta(n)
\end{align*}$$

The key observation is that the **for** loop dominates the running time, and it performs a constant number of operations per element. No matter what values are in the array, we always examine each element exactly once, making the running time $$\Theta(n)$$ in all cases (best, worst, and average).

{% capture note %}
This linear running time is crucial for quicksort's overall performance. If we can achieve balanced partitions, each level of recursion costs $$\Theta(n)$$ total, and with $$\Theta(\lg n)$$ levels, we get $$\Theta(n \lg n)$$ overall running time.
{% endcapture %}
{% include aside.html title='Why $$\Theta(n)$$ partitioning matters' %}
