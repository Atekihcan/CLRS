---
title:       Exercise 7.3-1
published:   2025-11-16 10:00
modified:    2025-11-16 10:00
keywords:    "randomized quicksort, expected running time, worst-case analysis"
description: "Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time?"
---

> Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time?

For randomized algorithms, worst-case analysis provides little useful information because it depends on an unlikely alignment of both the input and the random choices. Expected running time gives a more meaningful performance guarantee.

Consider $$\textsc{Randomized-Quicksort}$$. Its worst-case running time is still $$\Theta(n^2)$$, which can occur if we're extraordinarily unlucky with every random pivot choice. However, this worst case requires selecting the worst possible pivot (either the minimum or maximum element) at *every single level* of recursion. The probability of this happening is astronomically small, decreasing exponentially with the input size.

For an array of size $$n$$, the probability of selecting the worst pivot at one level is $$2/n$$ (either the minimum or the maximum). For this to happen at all levels, we'd need approximately $$\lg n$$ consecutive worst-case choices (if we're very unlucky) or up to $$n$$ levels (if we're catastrophically unlucky). Even in the more optimistic scenario:

$$\Pr[\text{worst case}] \approx \left(\frac{2}{n}\right)^{\lg n} = \frac{1}{n^{\lg n - 1}}$$

This probability is negligible, far smaller than the probability of hardware failure or cosmic rays flipping bits in memory.

**Expected running time is a guarantee:**

The expected running time of $$O(n \lg n)$$ for $$\textsc{Randomized-Quicksort}$$ holds for *every* input. Unlike deterministic quicksort, which has bad inputs (sorted arrays), randomized quicksort has no worst-case input. An adversary cannot construct an input that guarantees bad performance because the algorithm's behavior depends on random choices made during execution.

**Comparison with deterministic algorithms:**

For a deterministic algorithm like standard $$\textsc{Quicksort}$$, worst-case analysis is meaningful because specific inputs (like sorted arrays) always cause worst-case behavior. For randomized quicksort, no such inputs exist. The "worst case" is a probabilistic event that we can make arbitrarily unlikely.

Expected running time provides a practical, reliable performance bound that applies to all inputs, making it the appropriate measure for randomized algorithms.

{% capture note %}
This is why randomized algorithms are often preferred in practice: they trade a theoretical worst-case guarantee (which rarely matters) for a strong expected-case guarantee that holds regardless of input. The expected $$O(n \lg n)$$ performance of randomized quicksort is more useful than knowing deterministic quicksort has $$O(n \lg n)$$ best-case and $$\Theta(n^2)$$ worst-case performance.
{% endcapture %}
{% include aside.html title='Expected vs. worst-case: what matters in practice' %}
