---
title:       Exercise 12.2-8
published:   2025-11-16 10:00
modified:    2025-11-16 10:00
keywords:    "binary search tree, tree successor, path length"
description: "Prove that no matter what node we start at in a height-h binary search tree, k successive calls to TREE-SUCCESSOR take O(k + h) time."
---

> Prove that no matter what node we start at in a height-$$h$$ binary search tree, $$k$$ successive calls to $$\textsc{Tree-Successor}$$ take $$O(k + h)$$ time.

This exercise asks us to bound the total time for $$k$$ successive calls to $$\textsc{Tree-Successor}$$, starting from an arbitrary node.

### Proof

We will use a potential-based argument to track the total time across $$k$$ operations.

{% include ads.html %}

**Key observation:** Define the potential function $$\Phi$$ as the depth of the current node. Initially, the depth could be anywhere from 0 (root) to $$h$$ (deepest node).

For each call to $$\textsc{Tree-Successor}(x)$$:

**Case 1:** If $$x$$ has a right child, the successor is found by:
- Moving right once (down one level)
- Following left pointers to the minimum

The downward movement from $$x$$ to its successor $$y$$ can increase the depth by at most $$h$$ (in the worst case, we go from the root to a leaf). However, once we go down, we stay at a deeper level or go back up.

**Case 2:** If $$x$$ has no right child, the algorithm walks up the tree until finding an ancestor where we came from the left subtree.

Let us count more carefully using credits:

- Each downward edge traversal **costs 1** but we assign it **2 credits**: 1 for the traversal itself, and 1 that we save
- Each upward edge traversal **costs 1** and we pay for it using a saved credit from when we went down that edge earlier

Initially, we might be at depth $$d \le h$$. In the worst case, the first operation might traverse up to the root (cost $$d \le h$$) and then down to a deep node (cost $$\le h$$). But subsequent operations reuse the saved credits from downward traversals.

More precisely, across $$k$$ successive $$\textsc{Tree-Successor}$$ calls:

1. We visit $$k$$ nodes (the $$k$$ successors)
2. Each time we move down an edge, we "pay" for a potential future upward traversal
3. The total downward movement is bounded by $$k + h$$ because:
   - We visit $$k$$ different nodes
   - The initial node could be at depth $$h$$, requiring up to $$h$$ upward moves initially (if the first successor is an ancestor)
   - Each downward move to reach a successor counts toward $$k$$ node visits

**Detailed accounting:**

- Starting node is at depth $$\le h$$
- Over $$k$$ successor calls, we traverse a path through $$k$$ nodes
- The path can go up at most $$h$$ times initially (to reach root from starting node)
- The path goes down each time we follow right pointers and then left pointers
- Each of the $$k$$ nodes is visited once
- Total upward edges: $$\le h + k$$ (initial climb up to $$h$$, plus potential climbs between successors, but each climb is "paid for" by a previous descent)
- Total downward edges: $$\le k + h$$ (to visit $$k$$ successors, starting from depth $$\le h$$)

Total edge traversals: $$O(k + h)$$

Since each edge traversal takes $$O(1)$$ time, the total time is $$O(k + h)$$.

### Example

Consider a tree of height $$h = 3$$:

```
        15
       /  \
      6   18
     / \  / \
    3   7 17 20
```

Starting from node 3 (depth 2), making $$k = 3$$ successor calls:

1. $$\textsc{Tree-Successor}(3) = 6$$: Go up 1 edge (cost 1)
2. $$\textsc{Tree-Successor}(6) = 7$$: Go right 1, (cost 1)
3. $$\textsc{Tree-Successor}(7) = 15$$: Go up 2 edges (cost 2)

Total: 4 edge traversals = $$O(k + h) = O(3 + 3) = O(6)$$ âœ“

{% capture note %}
The $$O(k + h)$$ bound is tight. Consider starting at the leftmost node (minimum) of a skewed tree that forms a path of height $$h$$. The first successor call might need to traverse up $$h$$ edges. The subsequent $$k-1$$ calls visit nodes down the path, taking $$O(k)$$ time. Total: $$O(k + h)$$.

This result is a generalization of Exercise 12.2-7: when $$k = n-1$$ (visiting all successors) and we start from the minimum, we get $$O(n + h) = O(n)$$ since $$h < n$$.
{% endcapture %}
{% include aside.html title='Tightness of the bound' %}
